import math
import os
import torch
import torch.distributed as dist
import torch.nn.functional as F


# ==========================================
# 1. åŸºç¡€ç¯å¢ƒåˆå§‹åŒ–
# ==========================================
def init_dist():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    # ä»ç¯å¢ƒå˜é‡è¯»å– Rank ä¿¡æ¯ (torchrun è‡ªåŠ¨æ³¨å…¥)
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))

    # ç»‘å®šå½“å‰è¿›ç¨‹åˆ°æŒ‡å®šçš„ GPU
    torch.cuda.set_device(local_rank)
    device = torch.device(f"cuda:{local_rank}")

    # åˆå§‹åŒ– NCCL åç«¯
    if not dist.is_initialized():
        dist.init_process_group(
            backend="nccl",
            device_id=device
        )

    return rank, world_size, device, local_rank


# ==========================================
# 2. 3D å¹¶è¡Œ Group æ„å»ºé€»è¾‘
# ==========================================
class ParallelGroups:
    def __init__(self, tp_size, pp_size, dp_size, rank, world_size):
        assert world_size == tp_size * pp_size * dp_size, \
            f"World Size ({world_size}) != TP({tp_size}) * PP({pp_size}) * DP({dp_size})"

        self.tp_group = None
        self.dp_group = None
        self.pp_group = None  # PP é€šå¸¸ä¸éœ€è¦ AllReduceï¼Œä½†ä¸ºäº†å®Œæ•´æ€§åˆ—å‡ºé€»è¾‘

        print(f"[Rank {rank}] åˆå§‹åŒ– Group: TP={tp_size}, PP={pp_size}, DP={dp_size}")

        # --- æ„å»º TP Group (è¿ç»­åˆ‡åˆ†) ---
        # é€»è¾‘ï¼š[0,1], [2,3], [4,5], [6,7]
        num_tp_groups = world_size // tp_size
        for i in range(num_tp_groups):
            ranks = list(range(i * tp_size, (i + 1) * tp_size))
            group = dist.new_group(ranks)
            if rank in ranks:
                self.tp_group = group
                # ä»…åœ¨ Rank 0 æ‰“å°ä¸€æ¬¡æ‹“æ‰‘ç»“æ„
                if rank == 0:
                    print(f"  TP Group {i}: {ranks}")

        # --- æ„å»º DP Group (è·¨ PP çš„åŒä½åˆ‡åˆ†) ---
        # é€»è¾‘ï¼šæ­¥é•¿ä¸º tp_size * pp_sizeã€‚
        # Rank 0 (Stage0, TP0) <-> Rank 4 (Stage0, TP0) [DP Group 0]
        # Rank 1 (Stage0, TP1) <-> Rank 5 (Stage0, TP1) [DP Group 1]
        # Rank 2 (Stage1, TP0) <-> Rank 6 (Stage1, TP0) [DP Group 2] ...

        # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šDP ç»„è¿æ¥çš„æ˜¯â€œå®Œå…¨ç›¸åŒçš„æ¨¡å‹éƒ¨åˆ†â€ä½†åœ¨â€œä¸åŒçš„æ•°æ®å‰¯æœ¬â€ä¸Šçš„å¡
        # åœ¨ 3D å¹¶è¡Œä¸­ï¼Œé€šå¸¸ DP ç»´åº¦çš„ stride æ˜¯æœ€å¤§çš„ï¼Œæˆ–è€…å–å†³äº rank çš„æ’åˆ—æ–¹å¼ã€‚
        # è¿™é‡Œå‡è®¾ Rank æ’åˆ—é¡ºåºä¸ºï¼šDP -> PP -> TP (Megatron å¸¸è§æ–¹å¼)
        # ä½†ä¸ºäº†é€‚é…ç®€å•çš„ 0-7 çº¿æ€§æ’åˆ—ï¼Œæˆ‘ä»¬å‡è®¾æ’åˆ—æ˜¯ï¼š
        # Rank ID = dp_idx * (pp * tp) + pp_idx * (tp) + tp_idx

        stride = tp_size * pp_size
        num_dp_groups = stride  # æœ‰å¤šå°‘ä¸ªå¹¶è¡Œçš„æµæ°´çº¿/TPç»„åˆï¼Œå°±æœ‰å¤šå°‘ä¸ª DP ç»„

        for i in range(num_dp_groups):
            # i è¡¨ç¤º (PP_idx, TP_idx) çš„ç»„åˆç´¢å¼•
            ranks = [i + k * stride for k in range(dp_size)]
            group = dist.new_group(ranks)
            if rank in ranks:
                self.dp_group = group
                if rank == 0:
                    print(f"  DP Group (Base {i}): {ranks}")


# ==========================================
# 3. é€šç”¨ Ring AllReduce (æ”¯æŒä»»æ„ Group)
# ==========================================
def ring_all_reduce(tensor: torch.Tensor, group: dist.ProcessGroup = None) -> torch.Tensor:
    """
    é€šç”¨çš„ Ring AllReduce å®ç°
    :param tensor: è¾“å…¥å¼ é‡
    :param group: é€šä¿¡ç»„ (TPç»„ æˆ– DPç»„)
    """
    if group is None:
        group = dist.group.WORLD

    # 1. è·å–ç»„å†…é€»è¾‘ Rank (0 ~ group_size-1)
    rank_in_group = dist.get_rank(group)
    world_size_in_group = dist.get_world_size(group)

    if world_size_in_group == 1:
        return tensor

    # 2. é¢„å¤„ç†ï¼šFlatten + Padding
    original_shape = tensor.shape
    tensor_flat = tensor.flatten()
    numel = tensor_flat.numel()

    pad_len = (world_size_in_group - (numel % world_size_in_group)) % world_size_in_group
    if pad_len > 0:
        tensor_flat = F.pad(tensor_flat, (0, pad_len))

    # 3. åˆ†å—
    chunk_size = tensor_flat.numel() // world_size_in_group
    chunks = list(tensor_flat.split(chunk_size))

    # 4. è®¡ç®—ç¯å½¢é‚»å±… (é€»è¾‘ Rank -> ç‰©ç† Global Rank)
    right_rank_logical = (rank_in_group + 1) % world_size_in_group
    left_rank_logical = (rank_in_group - 1 + world_size_in_group) % world_size_in_group

    right_rank_global = dist.get_global_rank(group, right_rank_logical)
    left_rank_global = dist.get_global_rank(group, left_rank_logical)

    # 5. Reduce-Scatter
    for step in range(world_size_in_group - 1):
        send_idx = (rank_in_group - step + world_size_in_group) % world_size_in_group
        recv_idx = (rank_in_group - step - 1 + world_size_in_group) % world_size_in_group

        send_chunk = chunks[send_idx]
        recv_buffer = torch.empty_like(chunks[recv_idx])

        reqs = dist.batch_isend_irecv([
            dist.P2POp(dist.isend, send_chunk, right_rank_global, group=group),
            dist.P2POp(dist.irecv, recv_buffer, left_rank_global, group=group)
        ])
        for req in reqs: req.wait()
        chunks[recv_idx].add_(recv_buffer)

    # 6. All-Gather
    for step in range(world_size_in_group - 1):
        send_idx = (rank_in_group - step + 1 + world_size_in_group) % world_size_in_group
        recv_idx = (rank_in_group - step + world_size_in_group) % world_size_in_group

        send_chunk = chunks[send_idx]
        reqs = dist.batch_isend_irecv([
            dist.P2POp(dist.isend, send_chunk, right_rank_global, group=group),
            dist.P2POp(dist.irecv, chunks[recv_idx], left_rank_global, group=group)
        ])
        for req in reqs: req.wait()

    # 7. æ¢å¤å½¢çŠ¶
    res = torch.cat(chunks)
    if pad_len > 0:
        res = res[:-pad_len]
    return res.reshape(original_shape)


# ==========================================
# 4. ä¸»æµ‹è¯•é€»è¾‘
# ==========================================
def main():
    rank, world_size, device, local_rank = init_dist()

    # è®¾å®šå¹¶è¡Œåº¦ï¼šæ€»å¡æ•° 8 = 2(TP) * 2(PP) * 2(DP)
    TP_SIZE = 2
    PP_SIZE = 2
    DP_SIZE = 2

    # åˆå§‹åŒ– Group
    groups = ParallelGroups(TP_SIZE, PP_SIZE, DP_SIZE, rank, world_size)

    # ç®€å•çš„åŒæ­¥å±éšœï¼Œé˜²æ­¢æ‰“å°æ··ä¹±
    dist.barrier()
    if rank == 0:
        print("\n" + "=" * 50)
        print("ğŸš€ å¼€å§‹æµ‹è¯• 3D å¹¶è¡Œ Ring AllReduce")
        print("=" * 50 + "\n")
    dist.barrier()

    # -------------------------------------------------
    # æµ‹è¯•åœºæ™¯ 1: TP AllReduce (æ¨¡æ‹Ÿå‰å‘/åå‘ä¼ æ’­ä¸­çš„èšåˆ)
    # -------------------------------------------------
    # åªæœ‰åŒä¸€ä¸ª TP ç»„å†…çš„å¡ä¼šèšåˆã€‚
    # ä¾‹å¦‚ Rank 0 å’Œ Rank 1 èšåˆï¼›Rank 2 å’Œ Rank 3 èšåˆã€‚

    tensor_tp = torch.randn(1024, device=device) * (rank + 1)
    tensor_tp_ref = tensor_tp.clone()

    # æ‰§è¡Œæ‰‹å†™ Ring AllReduce
    res_tp = ring_all_reduce(tensor_tp, group=groups.tp_group)

    # æ‰§è¡Œå®˜æ–¹ AllReduce
    dist.all_reduce(tensor_tp_ref, op=dist.ReduceOp.SUM, group=groups.tp_group)

    # éªŒè¯è¯¯å·®
    err_tp = torch.mean((res_tp - tensor_tp_ref) ** 2)

    # æ‰“å°ç»“æœ (åªæ‰“å° Rank 0 å’Œ Rank 2ï¼Œä»£è¡¨ä¸åŒçš„ TP ç»„)
    if rank in [0, 2]:
        print(f"[TP Test] Rank {rank} (TP Group Rank {dist.get_rank(groups.tp_group)}): "
              f"Error = {err_tp.item():.5e} | "
              f"Val: {res_tp[0].item():.4f} vs Ref: {tensor_tp_ref[0].item():.4f}")

    dist.barrier()

    # -------------------------------------------------
    # æµ‹è¯•åœºæ™¯ 2: DP AllReduce (æ¨¡æ‹Ÿæ¢¯åº¦åŒæ­¥)
    # -------------------------------------------------
    # åŒä¸€ä¸ª DP ç»„å†…çš„å¡èšåˆã€‚
    # æ ¹æ®æˆ‘ä»¬çš„é€»è¾‘ï¼ŒRank 0 å’Œ Rank 4 æ˜¯ä¸€ä¸ª DP ç»„ã€‚

    tensor_dp = torch.randn(1024, device=device) + (rank + 10)
    tensor_dp_ref = tensor_dp.clone()

    # æ‰§è¡Œæ‰‹å†™ Ring AllReduce
    res_dp = ring_all_reduce(tensor_dp, group=groups.dp_group)

    # æ‰§è¡Œå®˜æ–¹ AllReduce
    dist.all_reduce(tensor_dp_ref, op=dist.ReduceOp.SUM, group=groups.dp_group)

    # éªŒè¯è¯¯å·®
    err_dp = torch.mean((res_dp - tensor_dp_ref) ** 2)

    # æ‰“å°ç»“æœ (åªæ‰“å° Rank 0 å’Œ Rank 1ï¼Œä»£è¡¨ä¸åŒçš„ DP ç»„åŸºåº•)
    if rank in [0, 1]:
        print(f"[DP Test] Rank {rank} (DP Group Rank {dist.get_rank(groups.dp_group)}): "
              f"Error = {err_dp.item():.5e} | "
              f"Val: {res_dp[0].item():.4f} vs Ref: {tensor_dp_ref[0].item():.4f}")

    dist.barrier()
    if rank == 0:
        print("\nâœ… æµ‹è¯•å®Œæˆï¼Œæ‰€æœ‰è¯¯å·®åº”åœ¨ 1e-10 çº§åˆ« (æµ®ç‚¹ç²¾åº¦è¯¯å·®)")

    # æ¸…ç†
    dist.destroy_process_group()


if __name__ == "__main__":
    main()
