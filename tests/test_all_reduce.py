import os
import torch
import torch.distributed as dist
from torch.distributed.device_mesh import init_device_mesh
from parallel.communication.all_reduce import ring_all_reduce

def init_dist():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))

    # 1. ç»‘å®šè®¾å¤‡
    torch.cuda.set_device(local_rank)
    device = torch.device(f"cuda:{local_rank}")

    # 2. åˆå§‹åŒ–é»˜è®¤è¿›ç¨‹ç»„ (è™½ç„¶ init_device_mesh å¯ä»¥è‡ªåŠ¨åˆå§‹åŒ–ï¼Œ
    #    ä½†æ˜¾å¼åˆå§‹åŒ–å¹¶æŒ‡å®š device_id æ˜¯æ¶ˆé™¤ Warning çš„æœ€ä½³å®è·µ)
    if not dist.is_initialized():
        dist.init_process_group(backend="nccl", device_id=device)

    return rank, world_size, device, local_rank


def main():
    rank, world_size, device, local_rank = init_dist()
    """
    # init_process_groupä¸æ˜¯å¿…é¡»çš„ï¼Œå¯ç”¨è¿™ä¸€æ®µæ¥ä»£æ›¿init_dist
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    torch.cuda.set_device(local_rank)
    device = torch.device(f"cuda:{local_rank}")
    """

    # è®¾å®šå¹¶è¡Œåº¦ï¼šæ€»å¡æ•° 8 = 2(DP) * 2(PP) * 2(TP)
    # æ³¨æ„ï¼šè¿™é‡Œçš„é¡ºåºå¾ˆé‡è¦ï¼Œå†³å®šäº† Rank å¦‚ä½•æ˜ å°„åˆ° Mesh
    # é€šå¸¸é¡ºåºæ˜¯ (Data, Pipeline, Tensor)
    mesh_shape = (2, 2, 2)
    mesh_dim_names = ("dp", "pp", "tp")

    # --- ä¸€é”®ç”Ÿæˆ 3D Mesh ---
    # è¿™è¡Œä»£ç è‡ªåŠ¨å®Œæˆäº†ä¹‹å‰å‡ åè¡Œçš„ Group åˆ›å»ºé€»è¾‘
    mesh_3d = init_device_mesh(
        "cuda",
        mesh_shape,
        mesh_dim_names=mesh_dim_names
    )

    # --- ç›´æ¥é€šè¿‡åå­—è·å– Group ---
    # è·å– TP ç»„ (æ²¿ç€ "tp" ç»´åº¦åˆ‡åˆ†)
    tp_group = mesh_3d["tp"].get_group()

    # è·å– DP ç»„ (æ²¿ç€ "dp" ç»´åº¦åˆ‡åˆ†)
    dp_group = mesh_3d["dp"].get_group()

    # ç®€å•çš„åŒæ­¥å±éšœ
    dist.barrier()
    if rank == 0:
        print("\n" + "=" * 50)
        print(f"ğŸš€ Device Mesh 3D å¹¶è¡Œæµ‹è¯• (Shape: {mesh_shape})")
        print("=" * 50 + "\n")
        # æ‰“å° Mesh ç»“æ„çœ‹çœ‹
        print(f"Mesh Structure:\n{mesh_3d}")

    dist.barrier()

    # -------------------------------------------------
    # æµ‹è¯•åœºæ™¯ 1: TP AllReduce
    # -------------------------------------------------
    tensor_tp = torch.randn(1024, device=device) * (rank + 1)
    tensor_tp_ref = tensor_tp.clone()

    # ä¼ å…¥ä» Mesh è·å–çš„ tp_group
    res_tp = ring_all_reduce(tensor_tp, group=tp_group)
    dist.all_reduce(tensor_tp_ref, op=dist.ReduceOp.SUM, group=tp_group)

    err_tp = torch.mean((res_tp - tensor_tp_ref) ** 2)

    if rank in [0, 1]:  # æ‰“å° Rank 0 å’Œ 1 (å®ƒä»¬åº”è¯¥åœ¨åŒä¸€ä¸ª TP ç»„)
        print(f"[TP Test] Rank {rank} (TP-Group Rank {dist.get_rank(tp_group)}): "
              f"Error = {err_tp.item():.5e}")

    dist.barrier()

    # -------------------------------------------------
    # æµ‹è¯•åœºæ™¯ 2: DP AllReduce
    # -------------------------------------------------
    tensor_dp = torch.randn(1024, device=device) + (rank + 10)
    tensor_dp_ref = tensor_dp.clone()

    # ä¼ å…¥ä» Mesh è·å–çš„ dp_group
    res_dp = ring_all_reduce(tensor_dp, group=dp_group)
    dist.all_reduce(tensor_dp_ref, op=dist.ReduceOp.SUM, group=dp_group)

    err_dp = torch.mean((res_dp - tensor_dp_ref) ** 2)

    if rank in [0, 4]:  # æ‰“å° Rank 0 å’Œ 4 (å®ƒä»¬åº”è¯¥åœ¨åŒä¸€ä¸ª DP ç»„)
        print(f"[DP Test] Rank {rank} (DP-Group Rank {dist.get_rank(dp_group)}): "
              f"Error = {err_dp.item():.5e}")

    print(f"\n\n _flatten_mesh_list: {mesh_3d._flatten_mesh_list}")

    dist.barrier()

    # æ¸…ç†
    dist.destroy_process_group()


if __name__ == "__main__":
    main()
