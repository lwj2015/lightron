import os
import torch
import torch.distributed as dist
from torch.distributed.device_mesh import init_device_mesh


def init_dist():
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))

    torch.cuda.set_device(local_rank)
    device = torch.device(f"cuda:{local_rank}")

    if not dist.is_initialized():
        dist.init_process_group(backend="nccl", device_id=device)

    return rank, world_size, device


def get_global_rank(group, group_rank):
    return dist.get_global_rank(group, group_rank)


def ring_reduce_scatter(tensor_list: list, group: dist.ProcessGroup) -> torch.Tensor:
    """
    ä¿®æ­£åçš„ Ring Reduce-Scatter
    é€»è¾‘ï¼šç¡®ä¿ Rank r æœ€ç»ˆæŒæœ‰ Chunk r çš„æ€»å’Œ
    """
    rank = dist.get_rank(group)
    world_size = dist.get_world_size(group)

    if world_size == 1:
        return tensor_list[0]

    # åˆå§‹åŒ–ï¼šResult åŒ…å«æˆ‘æœ¬åœ°çš„è´¡çŒ®
    # æ³¨æ„ï¼šæˆ‘ä»¬ç›´æ¥åœ¨ tensor_list ä¸ŠåŸåœ°ä¿®æ”¹ï¼Œè¿™æ ·æœ€å tensor_list[rank] å°±æ˜¯ç»“æœ

    right_rank_logical = (rank + 1) % world_size
    left_rank_logical = (rank - 1 + world_size) % world_size

    right_rank_global = get_global_rank(group, right_rank_logical)
    left_rank_global = get_global_rank(group, left_rank_logical)

    recv_buffer = torch.zeros_like(tensor_list[0])

    for i in range(world_size - 1):
        # ã€å…³é”®ä¿®æ­£ã€‘ç´¢å¼•åç§» -1
        # Step 0: Rank r å‘é€ Chunk r-1, æ¥æ”¶ Chunk r-2
        # è¿™æ ·ç»è¿‡ N-1 æ­¥ï¼ŒChunk r ä¼šæ­£å¥½ä¼ å›åˆ° Rank r

        send_chunk_idx = (rank - i - 1 + world_size) % world_size
        recv_chunk_idx = (rank - i - 2 + world_size) % world_size

        send_data = tensor_list[send_chunk_idx]

        reqs = dist.batch_isend_irecv([
            dist.P2POp(dist.isend, send_data, right_rank_global, group=group),
            dist.P2POp(dist.irecv, recv_buffer, left_rank_global, group=group)
        ])
        for req in reqs: req.wait()

        # ç´¯åŠ åˆ°å¯¹åº”çš„å—
        tensor_list[recv_chunk_idx] += recv_buffer

    # å¾ªç¯ç»“æŸåï¼Œtensor_list[rank] å·²ç»åŒ…å«äº†æ‰€æœ‰äººçš„è´¡çŒ®
    return tensor_list[rank]


def main():
    rank, world_size, device = init_dist()

    # æ„å»º 4D Device Mesh
    if world_size < 8:
        if rank == 0: print("âš ï¸ Warning: Less than 8 GPUs, using simplified mesh.")
        mesh_shape = (1, 1, 2, 2)
    else:
        mesh_shape = (2, 1, 2, 2)

    mesh_dim_names = ("dp", "pp", "tp", "cp")
    mesh = init_device_mesh("cuda", mesh_shape, mesh_dim_names=mesh_dim_names)

    if rank == 0:
        print(f"\nğŸš€ Device Mesh Created: {mesh_shape} {mesh_dim_names}")

    dist.barrier()

    # æµ‹è¯• Reduce-Scatter (åœ¨ DP ç»„å†…)
    dp_group = mesh["dp"].get_group()
    dp_world_size = dist.get_world_size(dp_group)

    input_list = [torch.ones(10, device=device) * (rank + 1) * (i + 1) for i in range(dp_world_size)]

    ref_out = torch.zeros(10, device=device)
    dist.reduce_scatter(ref_out, input_list, group=dp_group)

    input_list_2 = [torch.ones(10, device=device) * (rank + 1) * (i + 1) for i in range(dp_world_size)]
    my_out = ring_reduce_scatter(input_list_2, group=dp_group)

    diff = (ref_out - my_out).abs().max()
    if dist.get_rank(dp_group) == 0 and rank == 0:
        print(f"\n[DP Group] Reduce-Scatter Test:")
        print(f"   Max Diff: {diff.item():.6f} {'âœ…' if diff < 1e-5 else 'âŒ'}")

    dist.barrier()
    dist.destroy_process_group()


if __name__ == "__main__":
    main()
