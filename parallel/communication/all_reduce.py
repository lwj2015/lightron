import os
import torch
import torch.distributed as dist
import torch.nn.functional as F

# å¯¼å…¥ Device Mesh (PyTorch 2.x æ–°ç‰¹æ€§)
from torch.distributed.device_mesh import init_device_mesh


# ==========================================
# 1. åŸºç¡€ç¯å¢ƒåˆå§‹åŒ–
# ==========================================
def init_dist():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ (åœ°åŸº)"""
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))

    # 1. ç»‘å®šè®¾å¤‡
    torch.cuda.set_device(local_rank)
    device = torch.device(f"cuda:{local_rank}")

    # 2. åˆå§‹åŒ–é»˜è®¤è¿›ç¨‹ç»„ (è™½ç„¶ init_device_mesh å¯ä»¥è‡ªåŠ¨åˆå§‹åŒ–ï¼Œ
    #    ä½†æ˜¾å¼åˆå§‹åŒ–å¹¶æŒ‡å®š device_id æ˜¯æ¶ˆé™¤ Warning çš„æœ€ä½³å®è·µ)
    if not dist.is_initialized():
        dist.init_process_group(backend="nccl", device_id=device)

    return rank, world_size, device, local_rank


# ==========================================
# 2. é€šç”¨ Ring AllReduce (æ ¸å¿ƒé€»è¾‘å®Œå…¨ä¸å˜)
# ==========================================
def ring_all_reduce(tensor: torch.Tensor, group: dist.ProcessGroup = None) -> torch.Tensor:
    """
    é€šç”¨çš„ Ring AllReduce å®ç°
    :param tensor: è¾“å…¥å¼ é‡
    :param group: é€šä¿¡ç»„ (TPç»„ æˆ– DPç»„)
    """
    if group is None:
        group = dist.group.WORLD

    # è·å–ç»„å†…é€»è¾‘ Rank
    rank_in_group = dist.get_rank(group)
    world_size_in_group = dist.get_world_size(group)

    if world_size_in_group == 1:
        return tensor

    # é¢„å¤„ç†ï¼šFlatten + Padding
    original_shape = tensor.shape
    tensor_flat = tensor.flatten()
    numel = tensor_flat.numel()

    pad_len = (world_size_in_group - (numel % world_size_in_group)) % world_size_in_group
    if pad_len > 0:
        tensor_flat = F.pad(tensor_flat, (0, pad_len))

    # åˆ†å—
    chunk_size = tensor_flat.numel() // world_size_in_group
    chunks = list(tensor_flat.split(chunk_size))

    # è®¡ç®—ç¯å½¢é‚»å±… (é€»è¾‘ Rank -> ç‰©ç† Global Rank)
    right_rank_logical = (rank_in_group + 1) % world_size_in_group
    left_rank_logical = (rank_in_group - 1 + world_size_in_group) % world_size_in_group

    right_rank_global = dist.get_global_rank(group, right_rank_logical)
    left_rank_global = dist.get_global_rank(group, left_rank_logical)

    # Reduce-Scatter
    for step in range(world_size_in_group - 1):
        send_idx = (rank_in_group - step + world_size_in_group) % world_size_in_group
        recv_idx = (rank_in_group - step - 1 + world_size_in_group) % world_size_in_group

        send_chunk = chunks[send_idx]
        recv_buffer = torch.empty_like(chunks[recv_idx])

        reqs = dist.batch_isend_irecv([
            dist.P2POp(dist.isend, send_chunk, right_rank_global, group=group),
            dist.P2POp(dist.irecv, recv_buffer, left_rank_global, group=group)
        ])
        for req in reqs: req.wait()
        chunks[recv_idx].add_(recv_buffer)

    # All-Gather
    for step in range(world_size_in_group - 1):
        send_idx = (rank_in_group - step + 1 + world_size_in_group) % world_size_in_group
        recv_idx = (rank_in_group - step + world_size_in_group) % world_size_in_group

        send_chunk = chunks[send_idx]
        reqs = dist.batch_isend_irecv([
            dist.P2POp(dist.isend, send_chunk, right_rank_global, group=group),
            dist.P2POp(dist.irecv, chunks[recv_idx], left_rank_global, group=group)
        ])
        for req in reqs: req.wait()

    # æ¢å¤å½¢çŠ¶
    res = torch.cat(chunks)
    if pad_len > 0:
        res = res[:-pad_len]
    return res.reshape(original_shape)


# ==========================================
# 3. ä¸»æµ‹è¯•é€»è¾‘ (ä½¿ç”¨ init_device_mesh)
# ==========================================
def main():
    rank, world_size, device, local_rank = init_dist()
    """
    # init_process_groupä¸æ˜¯å¿…é¡»çš„ï¼Œå¯ç”¨è¿™ä¸€æ®µæ¥ä»£æ›¿init_dist
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    torch.cuda.set_device(local_rank)
    device = torch.device(f"cuda:{local_rank}")
    """

    # è®¾å®šå¹¶è¡Œåº¦ï¼šæ€»å¡æ•° 8 = 2(DP) * 2(PP) * 2(TP)
    # æ³¨æ„ï¼šè¿™é‡Œçš„é¡ºåºå¾ˆé‡è¦ï¼Œå†³å®šäº† Rank å¦‚ä½•æ˜ å°„åˆ° Mesh
    # é€šå¸¸é¡ºåºæ˜¯ (Data, Pipeline, Tensor)
    mesh_shape = (2, 2, 2)
    mesh_dim_names = ("dp", "pp", "tp")

    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¸€é”®ç”Ÿæˆ 3D Mesh ---
    # è¿™è¡Œä»£ç è‡ªåŠ¨å®Œæˆäº†ä¹‹å‰å‡ åè¡Œçš„ Group åˆ›å»ºé€»è¾‘
    mesh_3d = init_device_mesh(
        "cuda",
        mesh_shape,
        mesh_dim_names=mesh_dim_names
    )

    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç›´æ¥é€šè¿‡åå­—è·å– Group ---
    # è·å– TP ç»„ (æ²¿ç€ "tp" ç»´åº¦åˆ‡åˆ†)
    tp_group = mesh_3d["tp"].get_group()

    # è·å– DP ç»„ (æ²¿ç€ "dp" ç»´åº¦åˆ‡åˆ†)
    dp_group = mesh_3d["dp"].get_group()

    # ç®€å•çš„åŒæ­¥å±éšœ
    dist.barrier()
    if rank == 0:
        print("\n" + "=" * 50)
        print(f"ğŸš€ Device Mesh 3D å¹¶è¡Œæµ‹è¯• (Shape: {mesh_shape})")
        print("=" * 50 + "\n")
        # æ‰“å° Mesh ç»“æ„çœ‹çœ‹
        print(f"Mesh Structure:\n{mesh_3d}")

    dist.barrier()

    # -------------------------------------------------
    # æµ‹è¯•åœºæ™¯ 1: TP AllReduce
    # -------------------------------------------------
    tensor_tp = torch.randn(1024, device=device) * (rank + 1)
    tensor_tp_ref = tensor_tp.clone()

    # ä¼ å…¥ä» Mesh è·å–çš„ tp_group
    res_tp = ring_all_reduce(tensor_tp, group=tp_group)
    dist.all_reduce(tensor_tp_ref, op=dist.ReduceOp.SUM, group=tp_group)

    err_tp = torch.mean((res_tp - tensor_tp_ref) ** 2)

    if rank in [0, 1]:  # æ‰“å° Rank 0 å’Œ 1 (å®ƒä»¬åº”è¯¥åœ¨åŒä¸€ä¸ª TP ç»„)
        print(f"[TP Test] Rank {rank} (TP-Group Rank {dist.get_rank(tp_group)}): "
              f"Error = {err_tp.item():.5e}")

    dist.barrier()

    # -------------------------------------------------
    # æµ‹è¯•åœºæ™¯ 2: DP AllReduce
    # -------------------------------------------------
    tensor_dp = torch.randn(1024, device=device) + (rank + 10)
    tensor_dp_ref = tensor_dp.clone()

    # ä¼ å…¥ä» Mesh è·å–çš„ dp_group
    res_dp = ring_all_reduce(tensor_dp, group=dp_group)
    dist.all_reduce(tensor_dp_ref, op=dist.ReduceOp.SUM, group=dp_group)

    err_dp = torch.mean((res_dp - tensor_dp_ref) ** 2)

    if rank in [0, 4]:  # æ‰“å° Rank 0 å’Œ 4 (å®ƒä»¬åº”è¯¥åœ¨åŒä¸€ä¸ª DP ç»„)
        print(f"[DP Test] Rank {rank} (DP-Group Rank {dist.get_rank(dp_group)}): "
              f"Error = {err_dp.item():.5e}")
    
    print(f"\n\n _flatten_mesh_list: {mesh_3d._flatten_mesh_list}")

    dist.barrier()

    # æ¸…ç†
    dist.destroy_process_group()


if __name__ == "__main__":
    main()
