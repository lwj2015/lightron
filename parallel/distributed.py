import torch.distributed as dist
from torch.distributed.device_mesh import init_device_mesh

# å…¨å±€ Mesh ç®¡ç†å™¨
_DEVICE_MESH = None
_MESH_DIMS = {}


def setup_distributed(
        tp_size: int = 1,
        pp_size: int = 1,
        cp_size: int = 1,
        ep_size: int = 1,
        dp_size: int = 1,
):
    """
    åˆå§‹åŒ– 5D å¹¶è¡Œç¯å¢ƒã€‚

    å±‚çº§ç»“æ„ (Hierarchy):
    1. PP (Pipeline): æœ€å¤–å±‚ï¼Œé€šå¸¸è·¨æœºã€‚
    2. DP (Data): æ•°æ®å¹¶è¡Œå±‚ã€‚
       æ³¨æ„ï¼šEP (Expert) é€šå¸¸æ˜¯ DP çš„ä¸€ç§å˜ä½“ã€‚
       - å¦‚æœ ep_size == 1: çº¯ DPï¼Œæ‰€æœ‰ DP rank æ‹¥æœ‰ç›¸åŒçš„ MoE å‚æ•°ã€‚
       - å¦‚æœ ep_size == dp_size: çº¯ EPï¼Œæ‰€æœ‰ DP rank æ‹¥æœ‰ä¸åŒçš„ä¸“å®¶ã€‚
       - å¦‚æœ 1 < ep_size < dp_size: æ··åˆæ¨¡å¼ (Hybrid EP)ã€‚
    3. CP (Context): ä¸Šä¸‹æ–‡å¹¶è¡Œï¼Œåˆ‡åˆ† Sequenceã€‚
    4. TP (Tensor): æœ€å†…å±‚ï¼Œåˆ‡åˆ†ç®—å­ï¼Œé€šå¸¸åœ¨å•æœº NVLink èŒƒå›´å†…ã€‚
    """
    if not dist.is_initialized():
        dist.init_process_group("nccl")

    world_size = dist.get_world_size()
    rank = dist.get_rank()

    # 1. æ ¡éªŒ World Size
    # æ³¨æ„ï¼šEP ä¸å¢åŠ æ€» World Sizeï¼Œå®ƒæ˜¯å¯„ç”Ÿåœ¨ DP ç»´åº¦ä¸Šçš„
    # æ€»å¡æ•° = PP * DP * CP * TP
    expected_world_size = pp_size * dp_size * cp_size * tp_size

    if world_size != expected_world_size:
        raise ValueError(
            f"World Size Mismatch! Real: {world_size}, "
            f"Configured: {pp_size}(PP) * {dp_size}(DP) * {cp_size}(CP) * {tp_size}(TP) = {expected_world_size}"
        )

    # 2. æ ¡éªŒ EP åˆæ³•æ€§
    # EP æ˜¯åœ¨ DP ç»„å†…è¿›è¡Œçš„ï¼Œæ‰€ä»¥ ep_size å¿…é¡»èƒ½æ•´é™¤ dp_size
    if dp_size % ep_size != 0:
        raise ValueError(f"DP size ({dp_size}) must be divisible by EP size ({ep_size})")

    # 3. æ„å»º Device Mesh
    # ç»´åº¦é¡ºåºï¼š(PP, DP, CP, TP)
    mesh_dims = []
    mesh_names = []

    if pp_size > 1:
        mesh_dims.append(pp_size)
        mesh_names.append("pp")

    if dp_size > 1:
        mesh_dims.append(dp_size)
        mesh_names.append("dp")

    if cp_size > 1:
        mesh_dims.append(cp_size)
        mesh_names.append("cp")

    if tp_size > 1:
        mesh_dims.append(tp_size)
        mesh_names.append("tp")

    global _DEVICE_MESH, _MESH_DIMS

    if len(mesh_dims) > 0:
        _DEVICE_MESH = init_device_mesh("cuda", tuple(mesh_dims), mesh_dim_names=tuple(mesh_names))
    else:
        # å•å¡æ¨¡å¼
        _DEVICE_MESH = init_device_mesh("cuda", (1,), mesh_dim_names=("dp",))

    # 4. å­˜å‚¨é…ç½®ä¾›åç»­æŸ¥è¯¢
    _MESH_DIMS = {
        "tp": tp_size,
        "pp": pp_size,
        "cp": cp_size,
        "ep": ep_size,
        "dp": dp_size
    }

    if rank == 0:
        print(f"ğŸš€ Distributed Init Success!")
        print(f"   Shape: PP={pp_size} | DP={dp_size} (EP={ep_size}) | CP={cp_size} | TP={tp_size}")
        print(f"   Mesh: {mesh_names}")


def get_device_mesh():
    return _DEVICE_MESH


def get_parallel_info():
    return _MESH_DIMS


# === è·å–å„ä¸ªç»´åº¦çš„ Process Group ===

def get_tp_group():
    return _DEVICE_MESH["tp"].get_group() if "tp" in _DEVICE_MESH.mesh_dim_names else None


def get_cp_group():
    return _DEVICE_MESH["cp"].get_group() if "cp" in _DEVICE_MESH.mesh_dim_names else None


def get_pp_group():
    return _DEVICE_MESH["pp"].get_group() if "pp" in _DEVICE_MESH.mesh_dim_names else None


def get_dp_group():
    # çº¯ DP ç»„ (ç”¨äºåŒæ­¥é MoE å‚æ•°)
    return _DEVICE_MESH["dp"].get_group() if "dp" in _DEVICE_MESH.mesh_dim_names else None


def get_ep_group():
    """
    è·å– EP é€šä¿¡ç»„ã€‚
    EP æ¯”è¾ƒç‰¹æ®Šï¼Œå®ƒæ˜¯åœ¨ DP ç»´åº¦ä¸Šåˆ‡åˆ†çš„ã€‚
    å¦‚æœ ep_size == dp_sizeï¼Œé‚£ä¹ˆ EP group å°±æ˜¯ DP groupã€‚
    å¦‚æœ ep_size < dp_sizeï¼Œæˆ‘ä»¬éœ€è¦åœ¨ DP group å†…éƒ¨å†åˆ‡åˆ†ã€‚
    (ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œå‡è®¾ ep_size == dp_sizeï¼Œå³æ ‡å‡† MoE)
    """
    return get_dp_group()
