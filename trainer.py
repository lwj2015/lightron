import os
import json
import argparse
import time
import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.distributed as dist
from transformers import AutoConfig

from config.config import ModelArgs
from model.model import LightronTransformer
from parallel.distributed import setup_distributed
from parallel.parallel_fsdp import apply_fsdp2
from data.dataloader import MicroBatchDataLoader


def get_args():
    parser = argparse.ArgumentParser(description="Lightron Training Script")
    parser.add_argument("--config", type=str, required=True, help="Path to JSON config file")
    return parser.parse_args()


def load_config(config_path):
    with open(config_path, "r") as f:
        return json.load(f)


def train_step(model, batch, grad_acc_steps):
    """å•æ­¥è®­ç»ƒé€»è¾‘"""
    # æ•°æ®ç§»åŠ¨åˆ° GPU
    input_ids = batch["input_ids"].cuda()
    target_ids = batch["target_ids"].cuda()

    # Forward
    # æ³¨æ„ï¼šLightronTransformer è¿”å›çš„æ˜¯ [B, S, VocabSize]
    logits = model(input_ids)

    # Loss Calculation
    # Reshape: [B*S, V] vs [B*S]
    loss = F.cross_entropy(
        logits.view(-1, logits.size(-1)),
        target_ids.view(-1)
    )

    # Scale loss for gradient accumulation
    loss = loss / grad_acc_steps
    loss.backward()

    return loss.item() * grad_acc_steps


def main():
    # 1. è§£æå‚æ•°ä¸é…ç½®
    args = get_args()
    config = load_config(args.config)

    dist_cfg = config["distributed"]
    train_cfg = config["training"]
    model_cfg = config["model"]
    data_cfg = config["dataset"]

    # 2. åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ (4D Parallel Setup)
    # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å– (torchrun)ï¼Œå¦‚æœæ²¡è®¾åˆ™ç”¨ config çš„é»˜è®¤å€¼
    tp_size = int(os.environ.get("TP_SIZE", dist_cfg.get("tp_size", 1)))
    dp_size = int(os.environ.get("DP_SIZE", dist_cfg.get("dp_size", 1)))
    cp_size = int(os.environ.get("CP_SIZE", dist_cfg.get("cp_size", 1)))
    pp_size = int(os.environ.get("PP_SIZE", dist_cfg.get("pp_size", 1)))
    ep_size = int(os.environ.get("EP_SIZE", dist_cfg.get("ep_size", 1)))

    setup_distributed(
        tp_size=tp_size,
        pp_size=pp_size,
        cp_size=cp_size,
        ep_size=ep_size,
        dp_size=dp_size
    )

    local_rank = int(os.environ["LOCAL_RANK"])
    global_rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    torch.cuda.set_device(local_rank)

    if global_rank == 0:
        print(f"ğŸš€ Starting training with config: {args.config}")
        print(f"   World Size: {world_size} | TP={tp_size} DP={dp_size}")

    # 3. è‡ªåŠ¨åŠ è½½æ¨¡å‹é…ç½® (ä» HF)
    # ä½¿ç”¨ HF_ENDPOINT ç¯å¢ƒå˜é‡ç¡®ä¿å›½å†…èƒ½ä¸‹è½½
    if "HF_ENDPOINT" not in os.environ:
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

    if global_rank == 0:
        print(f"Loading model config from {model_cfg['name']}...")

    # è®©æ‰€æœ‰è¿›ç¨‹éƒ½åŠ è½½ Config (Config æ–‡ä»¶å¾ˆå°ï¼Œä¸ä¼šæœ‰å¹¶å‘é—®é¢˜)
    hf_config = AutoConfig.from_pretrained(model_cfg["name"], trust_remote_code=True)

    vocab_size = hf_config.vocab_size
    if tp_size > 1:
        # è®¡ç®—éœ€è¦å¡«å……å¤šå°‘æ‰èƒ½è¢« tp_size æ•´é™¤
        if vocab_size % tp_size != 0:
            new_vocab_size = ((vocab_size // tp_size) + 1) * tp_size
            if global_rank == 0:
                print(f"âš ï¸ Vocab size {vocab_size} is not divisible by TP={tp_size}.")
                print(f"   Padding vocab size to {new_vocab_size}...")
            vocab_size = new_vocab_size

    # 4. è½¬æ¢ä¸º Lightron ModelArgs
    # è‡ªåŠ¨æ˜ å°„ HF å‚æ•°åˆ° Lightron å‚æ•°
    model_args = ModelArgs(
        dim=hf_config.hidden_size,
        n_layers=hf_config.num_hidden_layers,
        n_heads=hf_config.num_attention_heads,
        n_kv_heads=getattr(hf_config, "num_key_value_heads", hf_config.num_attention_heads),
        vocab_size=vocab_size,
        max_seq_len=train_cfg["seq_length"],
        norm_eps=getattr(hf_config, "rms_norm_eps", 1e-5),
        # å¹¶è¡Œæ¨¡å¼ï¼šå¦‚æœ TP > 1ï¼Œå¼€å¯æ‰‹åŠ¨ TP
        # parallel_mode='manual_tp' if tp_size > 1 else 'fsdp2',
        tp_size=tp_size,
        cp_size=cp_size,
        # MoE é…ç½® (ä» config è¯»å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸º 1)
        moe_num_experts=model_cfg.get("moe_num_experts", 1),
        moe_topk=model_cfg.get("moe_topk", 2),
        moe_layer_freq=model_cfg.get("moe_layer_freq", 2)
    )

    # 5. åˆå§‹åŒ–æ¨¡å‹
    # ä½¿ç”¨ Meta Device åˆå§‹åŒ–ï¼Œç§’çº§æ„å»ºï¼Œä¸å æ˜¾å­˜
    with torch.device("meta"):
        model = LightronTransformer(model_args)

    # 6. åº”ç”¨å¹¶è¡Œç­–ç•¥
    # A. TP/CP/EP: å·²ç»åœ¨ model.py å†…éƒ¨é€šè¿‡ parallel_mode å¤„ç†äº†å±‚ç»“æ„
    # B. FSDP (DP): å¤„ç†å‰©ä½™çš„å‚æ•°åˆ‡åˆ†
    if dp_size > 1:
        # FSDP2 ä¼šè‡ªåŠ¨å¤„ç† Meta åˆ° Real çš„å‚æ•°åˆå§‹åŒ–
        # æ³¨æ„ï¼šå¦‚æœ TP>1ï¼Œè¿™é‡Œæ˜¯æ··åˆå¹¶è¡Œï¼ŒFSDP2 ä¼šåœ¨ DP ç»´åº¦åˆ‡åˆ†

        # 1. å…ˆåˆ‡åˆ† (æ­¤æ—¶è¿˜æ˜¯ Meta Tensor)
        model = apply_fsdp2(model)

        # 2. åˆ†é…ç‰©ç†æ˜¾å­˜ (Materialize), è¿™ä¼šåœ¨æ¯å¼ å¡ä¸Šåªåˆ†é…å®ƒè´Ÿè´£çš„é‚£ä¸€éƒ¨åˆ†å‚æ•° (Local Shard)
        model = model.to_empty(device="cuda")

        # 3. åˆå§‹åŒ–å‚æ•°æ•°å€¼
        # å› ä¸ºæ˜¯ Meta åˆå§‹åŒ–ï¼Œç°åœ¨æ˜¾å­˜é‡Œå…¨æ˜¯åƒåœ¾æ•°æ®ï¼Œå¿…é¡» reset
        # ä¸ºäº†ä¿è¯æ‰€æœ‰ DP Rank åˆå§‹æƒé‡ä¸€è‡´ï¼Œæˆ‘ä»¬éœ€è¦å›ºå®šéšæœºç§å­
        torch.manual_seed(42 + global_rank)  # æ³¨æ„ï¼šé€šå¸¸ DP éœ€è¦ç›¸åŒç§å­ï¼Œä½† FSDP2 è¿™ç§å±€éƒ¨åˆå§‹åŒ–æ¯”è¾ƒç‰¹æ®Š

        # æ›´ä¸¥è°¨çš„åšæ³•ï¼šè®¾ç½®ç›¸åŒçš„ç§å­ï¼Œè®©å¤§å®¶ç®—å‡ºä¸€æ ·çš„éšæœºæ•°ï¼ˆå¦‚æœåˆ‡åˆ†é€»è¾‘å…è®¸ï¼‰, æˆ–è€… Rank 0 åˆå§‹åŒ–åå¹¿æ’­ï¼ˆå¤ªæ…¢ï¼‰ã€‚
        # å¯¹äº FSDP2ï¼Œæœ€ç®€å•çš„åšæ³•æ˜¯ï¼šè®¾ç½®å…¨å±€ç»Ÿä¸€ç§å­ï¼Œç„¶åä¾é  reset_parameters
        torch.manual_seed(train_cfg.get("seed", 42))

        def init_weights(m):
            # å¦‚æœæ¨¡å—æœ‰è‡ªå®šä¹‰çš„é‡ç½®æ–¹æ³•ï¼ˆå¦‚ Linear, Embedding, æˆ–æˆ‘ä»¬çš„ ParallelLinearï¼‰
            if hasattr(m, 'reset_parameters'):
                m.reset_parameters()
            # å…œåº•é€»è¾‘ï¼šé’ˆå¯¹åŸç”Ÿ PyTorch å±‚
            elif isinstance(m, (torch.nn.Linear, torch.nn.Embedding)):
                m.reset_parameters()

        model.apply(init_weights)
    else:
        # çº¯ TP æ¨¡å¼æˆ–å•å¡æ¨¡å¼ï¼Œéœ€è¦æ‰‹åŠ¨ materialize
        model = model.to_empty(device="cuda")
        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)

    if global_rank == 0:
        # ç»Ÿè®¡å‚æ•°é‡ (FSDP ä¸‹å¯èƒ½ä¸å‡†ï¼Œä»…ä¾›å‚è€ƒ)
        try:
            param_count = sum(p.numel() for p in model.parameters())
            print(f"Model initialized. Total Parameters (Local/Meta): {param_count / 1e9:.2f}B")
        except:
            pass

    # 7. åˆå§‹åŒ– DataLoader
    # ä½¿ç”¨æˆ‘ä»¬åˆšåˆšæµ‹è¯•é€šè¿‡çš„ MicroBatchDataLoader
    dataloader = MicroBatchDataLoader(
        micro_batch_size=train_cfg["micro_batch_size"],
        seq_length=train_cfg["seq_length"],
        dataset_name=data_cfg["name"],
        tokenizer_name=model_cfg["name"],  # å¤ç”¨æ¨¡å‹åä½œä¸º tokenizer å
        grad_acc_steps=train_cfg["gradient_accumulation_steps"],
        num_workers=data_cfg.get("num_workers", 0),
        max_samples=train_cfg.get("max_samples", None),
        split=data_cfg.get("split", "train")
    )

    # 8. ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=train_cfg["learning_rate"],
        weight_decay=train_cfg.get("weight_decay", 0.01)
    )

    # 9. è®­ç»ƒå¾ªç¯
    model.train()
    total_steps = train_cfg["total_steps"]
    step = 0
    tokens_seen = 0

    start_time = time.time()

    # åˆ›å»ºè¿­ä»£å™¨
    data_iter = iter(dataloader)

    if global_rank == 0:
        print("\n=== Start Training ===")

    while step < total_steps:
        optimizer.zero_grad()
        loss_accum = 0.0

        # Gradient Accumulation Loop
        for _ in range(train_cfg["gradient_accumulation_steps"]):
            try:
                batch = next(data_iter)
            except StopIteration:
                # Epoch ç»“æŸï¼Œé‡æ–°å¼€å§‹
                data_iter = iter(dataloader)
                batch = next(data_iter)

            loss_val = train_step(model, batch, train_cfg["gradient_accumulation_steps"])
            loss_accum += loss_val

        # Optimizer Step
        # FSDP ä¼šè‡ªåŠ¨å¤„ç†æ¢¯åº¦åŒæ­¥
        optimizer.step()

        step += 1
        # è®¡ç®—ååé‡
        current_tokens = dataloader.global_batch_size * train_cfg["seq_length"]
        tokens_seen += current_tokens

        # Logging
        if global_rank == 0 and step % train_cfg.get("log_interval", 10) == 0:
            elapsed = time.time() - start_time
            tokens_per_sec = tokens_seen / elapsed
            print(f"Step {step}/{total_steps} | Loss: {loss_accum:.4f} | TPS: {tokens_per_sec:.2f} tokens/s")

    if global_rank == 0:
        print("Training Finished!")

    dist.destroy_process_group()


if __name__ == "__main__":
    main()
