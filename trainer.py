import os
import json
import argparse
import time
import datetime

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.distributed as dist
from transformers import AutoConfig

from config.config import ModelArgs
from model.model import LightronTransformer
from parallel.distributed import setup_distributed, get_device_mesh
from parallel.parallel_fsdp import apply_fsdp2

from parallel.pipeline_parallel import (
    PipelineParallel,
    train_step_pipeline_afab,
    train_step_pipeline_1f1b,
)
from parallel.pp_communications import get_pp_group_manager

from data.dataloader import MicroBatchDataLoader

from layers.layers import precompute_freqs_cis


def get_args():
    parser = argparse.ArgumentParser(description="Lightron Training Script")
    parser.add_argument("--config", type=str, required=True, help="Path to JSON config file")
    return parser.parse_args()


def load_config(config_path):
    with open(config_path, "r") as f:
        return json.load(f)


def get_group_rank(group):
    if group is None:
        return 0
    return dist.get_rank(group=group)


def train_step_single(model, batch, grad_acc_steps, device):
    input_ids = batch["input_ids"].to(device)
    target_ids = batch["target_ids"].to(device)
    logits = model(input_ids)  # [B, S, V]
    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_ids.view(-1), reduction="mean")
    loss = loss / grad_acc_steps
    loss.backward()
    return loss.item()


class InfiniteDataIterator:
    """
    è®© MicroBatchDataLoader next(data_loader) æ°¸ä¸ StopIteration å¹¶æš´éœ² grad_acc_stepsã€‚
    """
    def __init__(self, dataloader, grad_acc_steps):
        self.dataloader = dataloader
        self.grad_acc_steps = grad_acc_steps
        self._it = iter(dataloader)
    def __next__(self):
        try:
            return next(self._it)
        except StopIteration:
            self._it = iter(self.dataloader)
            return next(self._it)


def main():
    # 1. è§£æå‚æ•°ä¸é…ç½®
    args = get_args()
    config = load_config(args.config)

    dist_cfg = config["distributed"]
    train_cfg = config["training"]
    model_cfg = config["model"]
    data_cfg = config["dataset"]

    # 2. åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ (4D Parallel Setup)
    # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å– (torchrun)ï¼Œå¦‚æœæ²¡è®¾åˆ™ç”¨ config çš„é»˜è®¤å€¼
    tp_size = int(os.environ.get("TP_SIZE", dist_cfg.get("tp_size", 1)))
    dp_size = int(os.environ.get("DP_SIZE", dist_cfg.get("dp_size", 1)))
    cp_size = int(os.environ.get("CP_SIZE", dist_cfg.get("cp_size", 1)))
    pp_size = int(os.environ.get("PP_SIZE", dist_cfg.get("pp_size", 1)))
    ep_size = int(os.environ.get("EP_SIZE", dist_cfg.get("ep_size", 1)))

    local_rank = int(os.environ["LOCAL_RANK"])
    global_rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)

    dist.init_process_group(
        backend='nccl',
        init_method="env://",
        rank=global_rank,
        world_size=world_size,
        timeout=datetime.timedelta(minutes=10),
    )

    setup_distributed(
        tp_size=tp_size,
        pp_size=pp_size,
        cp_size=cp_size,
        ep_size=ep_size,
        dp_size=dp_size
    )

    mesh = get_device_mesh()

    if global_rank == 0:
        print(f"ğŸš€ Starting training with config: {args.config}")
        print(f"   World Size: {world_size} | TP={tp_size} DP={dp_size}")
    
    # PP + FSDP2 æš‚æ—¶å…ˆåˆ«æ··ï¼ˆåç»­å¯ä»¥åš dp_mesh slice + require_backward_grad_syncï¼‰
    if pp_size > 1:
        assert dp_size == 1, "å½“å‰è¿™ç‰ˆ Picotron-style PP trainer å…ˆè¦æ±‚ dp_size==1ï¼ˆæš‚ä¸ä¸ FSDP2 æ··ç”¨ï¼‰"

    # 3. è‡ªåŠ¨åŠ è½½æ¨¡å‹é…ç½® (ä» HF)
    # ä½¿ç”¨ HF_ENDPOINT ç¯å¢ƒå˜é‡ç¡®ä¿å›½å†…èƒ½ä¸‹è½½
    if "HF_ENDPOINT" not in os.environ:
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

    if global_rank == 0:
        print(f"Loading model config from {model_cfg['name']}...")

    # è®©æ‰€æœ‰è¿›ç¨‹éƒ½åŠ è½½ Config (Config æ–‡ä»¶å¾ˆå°ï¼Œä¸ä¼šæœ‰å¹¶å‘é—®é¢˜)
    hf_config = AutoConfig.from_pretrained(model_cfg["name"], trust_remote_code=True)

    vocab_size = hf_config.vocab_size
    if tp_size > 1:
        # è®¡ç®—éœ€è¦å¡«å……å¤šå°‘æ‰èƒ½è¢« tp_size æ•´é™¤
        if vocab_size % tp_size != 0:
            new_vocab_size = ((vocab_size // tp_size) + 1) * tp_size
            if global_rank == 0:
                print(f"âš ï¸ Vocab size {vocab_size} is not divisible by TP={tp_size}.")
                print(f"   Padding vocab size to {new_vocab_size}...")
            vocab_size = new_vocab_size

    # 4. è½¬æ¢ä¸º Lightron ModelArgs
    # è‡ªåŠ¨æ˜ å°„ HF å‚æ•°åˆ° Lightron å‚æ•°
    model_args = ModelArgs(
        dim=hf_config.hidden_size,
        n_layers=hf_config.num_hidden_layers,
        n_heads=hf_config.num_attention_heads,
        n_kv_heads=getattr(hf_config, "num_key_value_heads", hf_config.num_attention_heads),
        vocab_size=vocab_size,
        max_seq_len=train_cfg["seq_length"],
        norm_eps=getattr(hf_config, "rms_norm_eps", 1e-5),
        # å¹¶è¡Œæ¨¡å¼
        tp_size=tp_size,
        cp_size=cp_size,
        # MoE é…ç½®
        moe_num_experts=model_cfg.get("moe_num_experts", 1),
        moe_topk=model_cfg.get("moe_topk", 2),
        moe_layer_freq=model_cfg.get("moe_layer_freq", 2)
    )

    # 5. åˆå§‹åŒ–æ¨¡å‹
    # ä½¿ç”¨ Meta Device åˆå§‹åŒ–ï¼Œç§’çº§æ„å»ºï¼Œä¸å æ˜¾å­˜
    with torch.device("meta"):
        base_model = LightronTransformer(model_args)

    # 6. åº”ç”¨å¹¶è¡Œç­–ç•¥
    # A. TP/CP/EP: å·²ç»åœ¨ model.py å†…éƒ¨é€šè¿‡ parallel_mode å¤„ç†äº†å±‚ç»“æ„
    # B. FSDP (DP): å¤„ç†å‰©ä½™çš„å‚æ•°åˆ‡åˆ†
    if dp_size > 1:
        # FSDP2 ä¼šè‡ªåŠ¨å¤„ç† Meta åˆ° Real çš„å‚æ•°åˆå§‹åŒ–
        # æ³¨æ„ï¼šå¦‚æœ TP>1ï¼Œè¿™é‡Œæ˜¯æ··åˆå¹¶è¡Œï¼ŒFSDP2 ä¼šåœ¨ DP ç»´åº¦åˆ‡åˆ†

        # 1. å…ˆåˆ‡åˆ† (æ­¤æ—¶è¿˜æ˜¯ Meta Tensor)
        base_model = apply_fsdp2(base_model)

        # 2. åˆ†é…ç‰©ç†æ˜¾å­˜ (Materialize), è¿™ä¼šåœ¨æ¯å¼ å¡ä¸Šåªåˆ†é…å®ƒè´Ÿè´£çš„é‚£ä¸€éƒ¨åˆ†å‚æ•° (Local Shard)
        base_model = base_model.to_empty(device="cuda")

        # 3. åˆå§‹åŒ–å‚æ•°æ•°å€¼
        # å› ä¸ºæ˜¯ Meta åˆå§‹åŒ–ï¼Œç°åœ¨æ˜¾å­˜é‡Œå…¨æ˜¯åƒåœ¾æ•°æ®ï¼Œå¿…é¡» reset
        # ä¸ºäº†ä¿è¯æ‰€æœ‰ DP Rank åˆå§‹æƒé‡ä¸€è‡´ï¼Œæˆ‘ä»¬éœ€è¦å›ºå®šéšæœºç§å­
        torch.manual_seed(42 + global_rank)  # æ³¨æ„ï¼šé€šå¸¸ DP éœ€è¦ç›¸åŒç§å­ï¼Œä½† FSDP2 è¿™ç§å±€éƒ¨åˆå§‹åŒ–æ¯”è¾ƒç‰¹æ®Š

        # æ›´ä¸¥è°¨çš„åšæ³•ï¼šè®¾ç½®ç›¸åŒçš„ç§å­ï¼Œè®©å¤§å®¶ç®—å‡ºä¸€æ ·çš„éšæœºæ•°ï¼ˆå¦‚æœåˆ‡åˆ†é€»è¾‘å…è®¸ï¼‰, æˆ–è€… Rank 0 åˆå§‹åŒ–åå¹¿æ’­ï¼ˆå¤ªæ…¢ï¼‰ã€‚
        # å¯¹äº FSDP2ï¼Œæœ€ç®€å•çš„åšæ³•æ˜¯ï¼šè®¾ç½®å…¨å±€ç»Ÿä¸€ç§å­ï¼Œç„¶åä¾é  reset_parameters
        torch.manual_seed(train_cfg.get("seed", 42))

        def init_weights(m):
            # å¦‚æœæ¨¡å—æœ‰è‡ªå®šä¹‰çš„é‡ç½®æ–¹æ³•ï¼ˆå¦‚ Linear, Embedding, æˆ–æˆ‘ä»¬çš„ ParallelLinearï¼‰
            if hasattr(m, 'reset_parameters'):
                m.reset_parameters()
            # å…œåº•é€»è¾‘ï¼šé’ˆå¯¹åŸç”Ÿ PyTorch å±‚
            elif isinstance(m, (torch.nn.Linear, torch.nn.Embedding)):
                m.reset_parameters()

        base_model.apply(init_weights)
    else:
        # çº¯ TP æ¨¡å¼æˆ–å•å¡æ¨¡å¼ï¼Œéœ€è¦æ‰‹åŠ¨ materialize
        base_model = base_model.to_empty(device="cuda")
        base_model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)

    # recompute RoPE
    if global_rank == 0:
        print("Re-computing RoPE frequencies for Meta-initialized model...")
    with torch.no_grad():
        # é‡æ–°è®¡ç®—
        real_freqs = precompute_freqs_cis(
            model_args.dim // model_args.n_heads,
            model_args.max_seq_len
        )
        # ç§»åŠ¨åˆ° GPU å¹¶èµ‹å€¼ç»™æ¨¡å‹çš„ buffer
        base_model.freqs_cis.copy_(real_freqs.to("cuda"))

    if global_rank == 0:
        # ç»Ÿè®¡å‚æ•°é‡ (FSDP ä¸‹å¯èƒ½ä¸å‡†ï¼Œä»…ä¾›å‚è€ƒ)
        try:
            param_count = sum(p.numel() for p in base_model.parameters())
            print(f"Model initialized. Total Parameters (Local/Meta): {param_count / 1e9:.2f}B")
        except:
            pass
    
    # wrap PP (Picotron-style)
    if pp_size > 1:
        model = PipelineParallel(base_model, model_args)
    else:
        model = base_model

    model = model.to(torch.bfloat16)
    model.train()

    # 7. åˆå§‹åŒ– DataLoader
    # ä½¿ç”¨æˆ‘ä»¬åˆšåˆšæµ‹è¯•é€šè¿‡çš„ MicroBatchDataLoader
    dataloader = MicroBatchDataLoader(
        micro_batch_size=train_cfg["micro_batch_size"],
        seq_length=train_cfg["seq_length"],
        dataset_name=data_cfg["name"],
        tokenizer_name=model_cfg["name"],  # å¤ç”¨æ¨¡å‹åä½œä¸º tokenizer å
        grad_acc_steps=train_cfg["gradient_accumulation_steps"],
        num_workers=data_cfg.get("num_workers", 0),
        max_samples=train_cfg.get("max_samples", None),
        split=data_cfg.get("split", "train")
    )
    data_iter = InfiniteDataIterator(dataloader, train_cfg["gradient_accumulation_steps"])

    # 8. ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=train_cfg["learning_rate"],
        weight_decay=train_cfg.get("weight_decay", 0.01)
    )

    # tensor_shapes for PP comm (activation/grad between stages)
    # æ³¨æ„ï¼šPP ä¹‹é—´ä¼ çš„æ˜¯ hidden states: [B_micro, S_local(cp), H]
    seq_len_global = train_cfg["seq_length"]
    assert seq_len_global % cp_size == 0, "seq_length must be divisible by cp_size"

    seq_len_per_gpu = dataloader.seq_length_per_gpu
    tensor_shapes = (train_cfg["micro_batch_size"], seq_len_per_gpu, model_args.dim)

    ppm = get_pp_group_manager()
    is_log_rank = (global_rank == 0)  # ä½ ä¹Ÿå¯ä»¥æ”¹æˆï¼štp_rank==0 && cp_rank==0 && pp_last ç­‰
    total_steps = train_cfg["total_steps"]
    start_time = time.time()
    tokens_seen = 0
    if is_log_rank:
        print("\n=== Start Training ===")

    # 9. è®­ç»ƒå¾ªç¯
    # model.train()

    for step in range(1, total_steps + 1):
        optimizer.zero_grad(set_to_none=True)
        if pp_size > 1:
            engine = dist_cfg.get("pp_engine", "1f1b").lower()
            # è®© pipeline é‡Œçš„æ¯ä¸ª microbatch éƒ½èƒ½æ‹¿åˆ° batch
            # æ³¨æ„ï¼šå„ rank éƒ½ next(data_iter) ä»¥ä¿æŒæ•°æ®æµä¸€è‡´ï¼ˆæ¨¡ä»¿ picotronï¼‰
            if engine == "afab":
                loss = train_step_pipeline_afab(model, data_iter, tensor_shapes, device, torch.bfloat16)
            elif engine == "1f1b":
                loss = train_step_pipeline_1f1b(model, data_iter, tensor_shapes, device, torch.bfloat16)
            else:
                raise ValueError(f"Invalid pp_engine: {engine}")
            # é last stage çš„ loss é€šå¸¸æ˜¯ 0.0ï¼ˆä½ çš„ pipeline_parallel é‡Œå°±æ˜¯è¿™ä¹ˆåšçš„ï¼‰
        else:
            # non-PP path: normal grad accumulation
            loss = 0.0
            for i in range(train_cfg["gradient_accumulation_steps"]):
                batch = next(data_iter)
                # batch, _ = maybe_slice_for_cp(batch, cp_size, mesh)
                assert batch["input_ids"].shape[1] == dataloader.seq_length_per_gpu, \
                    f"expected S_local={dataloader.seq_length_per_gpu}, got {batch['input_ids'].shape[1]}"
                loss += train_step_single(model, batch, train_cfg["gradient_accumulation_steps"], device)
        optimizer.step()
        # throughputç»Ÿè®¡ï¼štokens_per_step ç”¨ global batchï¼ˆä½ çš„ dataloader.global_batch_sizeï¼‰æ›´åˆç†
        tokens_per_step = dataloader.global_batch_size * seq_len_global
        tokens_seen += tokens_per_step
        if is_log_rank and step % train_cfg.get("log_interval", 10) == 0:
            elapsed = time.time() - start_time
            tps = tokens_seen / elapsed
            print(f"Step {step}/{total_steps} | Loss: {loss:.4f} | TPS: {tps:.2f} tokens/s")

    if is_log_rank:
        # save checkpoint
        checkpoint = {
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'trained_steps': step,
            'trained_tokens': tokens_seen
        }
        torch.save(checkpoint, f'./ckpt_step_{step}_tpsize_{tp_size}_dpsize_{dp_size}')

    if is_log_rank == 0:
        print("Training Finished!")

    dist.destroy_process_group()


if __name__ == "__main__":
    main()
